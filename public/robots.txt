# Coldop Website - Robots.txt
# This file tells search engine crawlers which pages or files the crawler can or can't request from your site.

# Default rules for all crawlers
User-agent: *

# Allow public pages and assets
Allow: /
Allow: /faq
Allow: /privacy
Allow: /support
Allow: /case-studies
Allow: /sitemap.xml
Allow: /robots.txt
Allow: /*.css
Allow: /*.js
Allow: /coldop-logo.png
Allow: /favicon.ico
Allow: /app-screen-*.png
Allow: /hero-min.png

# Block sensitive and internal paths
Disallow: /login/
Disallow: /signup/
Disallow: /erp/
Disallow: /admin/
Disallow: /api/
Disallow: /src/
Disallow: /node_modules/
Disallow: /build/
Disallow: /dist/
Disallow: /coverage/
Disallow: /tests/
Disallow: /*.json
Disallow: /*.ts
Disallow: /*.tsx
Disallow: /*.map
Disallow: /*.config.js
Disallow: /*.lock
Disallow: /.env*
Disallow: /.git*

# Crawl-delay directive (in seconds)
Crawl-delay: 2

# Rules for specific search engines
User-agent: Googlebot
Allow: /
Disallow: /login/
Disallow: /signup/
Disallow: /erp/

User-agent: Bingbot
Allow: /
Disallow: /login/
Disallow: /signup/
Disallow: /erp/

User-agent: DuckDuckBot
Allow: /
Disallow: /login/
Disallow: /signup/
Disallow: /erp/

# Block known aggressive or unwanted bots
User-agent: AhrefsBot
Disallow: /

User-agent: MJ12bot
Disallow: /

User-agent: SemrushBot
Disallow: /

User-agent: PetalBot
Disallow: /

User-agent: ZoominfoBot
Disallow: /

User-agent: ChatGPT-User
Disallow: /

# Sitemap declaration
Sitemap: https://coldop.in/sitemap.xml